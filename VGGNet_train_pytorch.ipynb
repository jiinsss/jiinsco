{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGGNet_train_pytorch.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN8kn31N86hjcMDIKgxiWRN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"THXLaMk29hO4"},"outputs":[],"source":["from __future__ import print_function, division\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import lr_scheduler\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","import numpy as np\n","import torchvision\n","from torchvision import models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","\n","plt.ion()   # interactive mode\n"]},{"cell_type":"code","source":["#학습에서 사용되는 수치들\n","#Batch size \n","batch_size=128 #모두 로드를 할 수 없으니 한번에 배치사이즈만큼 가져옴\n","#Epoch  \n","num_epochs=3 #모델이 훈련에 얼마만큼 데이터를 받는지\n","#learning rate\n","learning_rate=0.001 #학습률이 높을수록 빠르게 학습할 수 있지만 학습이 다른 쪽으로 갈 수도 있음(신중하지 못함)"],"metadata":{"id":"jNsLyYCE9o8i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_transforms = {\n","    'train': transforms.Compose([ #train 전처리\n","        transforms.RandomResizedCrop(224), #224사이즈로 크롭\n","        transforms.RandomHorizontalFlip(), \n","        transforms.ToTensor(), #들어온 이미지에 대해 텐서로 변경\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화\n","    ]),\n","    'val': transforms.Compose([ #val - test set 전처리\n","        transforms.Resize(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","train_set=torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=data_transforms['train']) #cifar10 로드\n","test_set =torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=data_transforms['val'])\n","\n","dataloaders=dict()\n","dataloaders['train']= torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True) #데이터로더로 가져옴 train과\n","dataloaders['val']= torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False) #test, test에서는 데이터 순서를 셔플 해 줄 필요 없음\n","\n","dataset_sizes = {x: len(dataloaders[x].dataset) for x in ['train', 'val']}\n","\n","print(\"train 개수\",dataset_sizes['train'])\n","print(\"test 개수\",dataset_sizes['val'])\n","\n","class_names = train_set.classes\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(\"class_names:\",class_names) #cifar10의 클래스명 airplain.. 이 들어감\n","print(device)"],"metadata":{"id":"V__aXKoZ9thQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def imshow(inp, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\" #텐서 형태로 된 이미지를 볼 수 있는 함수\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","\n","# Get a batch of training data\n","inputs, classes = next(iter(dataloaders['train'])) #몇개 가져와서 imshow로 확인해보기\n","#batch가 너무 크면 다 안보이니 3개만 가져오기\n","inputs_=inputs[:3]\n","classes_=classes[:3]\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs_)\n","\n","imshow(out, title=[class_names[x] for x in classes_])"],"metadata":{"id":"QOKWfSOX9x1o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=25): #모델을 실제로 train 하는 코드\n","    since = time.time() #모델이 학습할 때 얼마나 걸리는지 계산함\n","\n","    best_model_wts = copy.deepcopy(model.state_dict()) #모델의 가중치에 대한 정보들이 담긴state dict를 깊은 복사를 해옴, \n","    #모델이 제일 좋은 성능을 보일때 가중치를 저장\n","    best_acc = 0.0 #best accuracy\n","\n","    train_loss_list=[]\n","    val_acc_list=[]\n","\n","    for epoch in range(num_epochs): # num epoochs만큼 반복학습\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","#파이토치에서는 epoch마다 train과 성능 evaluation\n","        # Each epoch has a training and validation phase \n","        for phase in ['train', 'val']: # train을 하고나서 train 된 모델에 대해 test하기 위해서 val이 들어감\n","            if phase == 'train':\n","                model.train()  # Set model to training mode 학습으로 모델 모드 변경\n","            else:\n","                model.eval()   # Set model to evaluate mode 평가로 모델 모드 변경\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","            iteration_count=0\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]: #input이미지, labels클래스 #phase가 val 로 test를 함\n","                iteration_count+=len(inputs)\n","                print('Iteration {}/{}'.format(iteration_count,dataset_sizes[phase]))\n","                inputs = inputs.to(device)\n","                labels = labels.to(device) #input과 labels를 gpu상에 올려서 연산이 가능하도록 만들어줌\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad() #그래디언트 파라미터를 0으로 만들어줌\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1) #예측 확률이(얼마나 맞췄는지) 가장 높은게 저장됨\n","                    loss = criterion(outputs, labels) #로스를 구함\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train': #훈련상태면 (test는 loss가 사용 안되니까 loss갱신 필요 업음)\n","                        loss.backward() #구한 로스를 각 노드들로 전달\n","                        optimizer.step() #optimizer로 가중치를 갱신 \n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            if phase == 'train':\n","                scheduler.step() #스케쥴러에 대해서도 step으로 갱신 ex) 7epoch마다 0.1씩 곱해줘서 학습률을 바꿔줬는데 이 스케줄러로 갱신됨\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase] #에폭에 대한 loss와 accuracy\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                phase, epoch_loss, epoch_acc))\n","            \n","            if phase==\"train\":\n","              train_loss_list.append(epoch_loss) #얼마나 차이가 나타나는지\n","            elif phase==\"val\":\n","              val_acc_list.append(epoch_acc)\n","\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc: #기존보다 test의 정확도가 더 좋았다면\n","                best_acc = epoch_acc #best를 현재로 갱신해줌\n","                best_model_wts = copy.deepcopy(model.state_dict()) #현재 모델의 가중치 정보도 넣어줌\n","\n","        print()\n","\n","    time_elapsed = time.time() - since #학습에 소요된 시간\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts) #가장 좋은 성능을 보였던 모델의 가중치를 로드해줌\n","    return model,train_loss_list,val_acc_list"],"metadata":{"id":"g5xhqeKf93f5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_model(model, num_images=6):\n","    was_training = model.training\n","    model.eval()\n","    images_so_far = 0\n","    fig = plt.figure()\n","\n","    with torch.no_grad():\n","        for i, (inputs, labels) in enumerate(dataloaders['val']):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","\n","            for j in range(inputs.size()[0]):\n","                images_so_far += 1\n","                ax = plt.subplot(num_images//2, 2, images_so_far)\n","                ax.axis('off')\n","                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n","                imshow(inputs.cpu().data[j])\n","\n","                if images_so_far == num_images:\n","                    model.train(mode=was_training)\n","                    return\n","        model.train(mode=was_training)"],"metadata":{"id":"k037FYbQ98-s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_ft = models.vgg16(pretrained=True) #앞서 model에서작성한 vgg16을 호출, 사전학습이 된 가중치를 vgg16에 로드해서 가져옴\n","num_ftrs = model_ft.classifier[6].in_features #6번째 in_features인 4096을 가져옴 , imagenet이 아닌 cifar10으로 할거임\n","model_ft.classifier[6] = nn.Linear(num_ftrs, len(class_names)) #클래스 개수에 맞춰 output 디맨션 맞춰줌\n","model_ft = model_ft.to(device) #device는 주로gpu\n","print(model_ft)\n","#여기까지 모델의 최종 출력층을 바꿔줬다면\n","\n","\n","criterion = nn.CrossEntropyLoss() #LOSS FUNC을 어떤걸 사용할 지 정의\n","\n","# Observe that all parameters are being optimized\n","#SGD\n","optimizer_ft = optim.SGD(model_ft.parameters(), lr=learning_rate, momentum=0.9) #옵티마이저로 sgd 사용\n","#Adam\n","# optimizer_ft = optim.Adam(model_ft.parameters(), lr=learning_rate)\n","\n","# Decay LR by a factor of 0.1 every 7 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) #lr ~ stepLR을 사용해 학습률을 줄임. 감마값 0.1씩을 곱해주면서 바꿔줌"],"metadata":{"id":"eRj5vNVL-Ayv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_ft,train_loss_list,val_acc_list = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n","                       num_epochs=num_epochs) #train_model로 몇에폭을 학습할지 등등을 정해 학습함"],"metadata":{"id":"49BHHjB9-EdK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#plot train loss \n","x=[i for i in range(0,num_epochs)]\n","plt.title(\"Train Loss\")\n","plt.xticks(x)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.plot(x,train_loss_list) \n","plt.show()\n","\n","#plot test acc\n","x=[i for i in range(0,num_epochs)]\n","plt.title(\"Test Accuracy\")\n","plt.xticks(x)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.plot(x,val_acc_list)\n","plt.show()"],"metadata":{"id":"Fl-B2zVm-JCR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize_model(model_ft)"],"metadata":{"id":"v4cefR8S-QJv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"u8zB4fbG-UT1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ibo1iLcsZsS8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"O4tFysd7kfm6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!jupyter nbconvert --to html \"/content/drive/MyDrive/Colab Notebooks/VGGNet_train_pytorch.ipynb\""],"metadata":{"id":"kBmCRzkEeVM3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"FD3vvwjKlN6_"},"execution_count":null,"outputs":[]}]}